# ğŸ•·ï¸ Concurrent Web Scraper Microservice

A high-performance, configurable microservice for concurrent web scraping. It exposes three interfaces:

* ğŸŒ A **user-friendly web page** for viewing and submitting scraping tasks
* ğŸ“¡ A **gRPC API** for fast, typed interactions
* ğŸ“¥ A **RESTful HTTP API** via Echo server for browser/client-side access

---

## ğŸš€ Features

* âš™ï¸ **Concurrent Scraping** with configurable concurrency, timeouts, and retries
* ğŸ“Š **Statistics & Metrics** tracking processed, successful, and failed tasks
* ğŸ“¦ **In-memory Caching** to reduce redundant scraping
* ğŸ”— **Multiple Selectors per Task** for fine-grained extraction
* ğŸ§ª **gRPC & REST APIs** for integration and control
* ğŸ–¥ï¸ **Interactive Web Interface** for task submission and result viewing

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ cmd/                # Entry point of the scraper
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ config/         # Configuration loader and defaults
â”‚   â”œâ”€â”€ scraper/        # Scraper core logic
â”‚   â””â”€â”€ server/         # gRPC and Echo server implementations
â”œâ”€â”€ pkg/types/          # Shared types, config, and protobuf definitions
â”‚   â””â”€â”€ proto/          # .proto files for gRPC
â”œâ”€â”€ bin/                # Compiled binary output
â”œâ”€â”€ Makefile            # Build, test, proto generation
â””â”€â”€ README.md           # You're here!
```

---

## âš™ï¸ Installation & Usage

### ğŸ”¨ Build the project

```bash
make build
```

### âœ… Run tests

```bash
make test
```

### ğŸ³ Build & Run in Docker

```bash
make docker-build
make docker-run
```

* gRPC API available at `localhost:8080`
* REST API (Echo server) at `localhost:8081`

---

## ğŸ§ª API Endpoints

### gRPC

* **Service**: `ScraperService`
* **Methods**:

  * `AddTask(ScrapingTask)`
  * `GetResults(Empty)`
  * `GetStatistics(Empty)`

> Define your gRPC client using `pkg/types/proto/scraper.proto`

### REST (Echo)

* `POST /tasks` â€“ Add new scraping task
* `GET /results` â€“ Get completed results
* `GET /stats` â€“ Fetch runtime statistics

---

## ğŸ“„ Scraping Task Structure

```json
{
  "url": "http://example.com",
  "selectors": {
    "title": "h1",
    "description": ".desc"
  },
  "headers": {
    "Authorization": "Bearer token"
  },
  "method": "GET"
}
```

---

## ğŸ“¦ Configuration

Supports JSON config via `-config` flag:

```json
{
  "max_concurrency": 10,
  "rate_limit": 2.5,
  "timeout": "5s",
  "retry_attempts": 3,
  "retry_delay": "1s",
  "cache_ttl": "30m",
  "user_agent": "MyScraperBot/1.0",
  "max_body_size": 1048576
}
```

Or use built-in defaults.

---

## ğŸ“œ Makefile Targets

| Target              | Description                             |
| ------------------- | --------------------------------------- |
| `make all`          | Clean, format, test, build              |
| `make build`        | Build the binary in `bin/scraper`       |
| `make test`         | Run all tests                           |
| `make fmt`          | Format Go code                          |
| `make clean`        | Remove binaries                         |
| `make proto`        | Generate Go code from proto definitions |
| `make docker-build` | Build Docker image                      |
| `make docker-run`   | Run container with ports exposed        |

---

## ğŸ›  Development

To regenerate gRPC stubs:

```bash
make proto
```

---

## ğŸ§¹ Graceful Shutdown

Handles `SIGINT` and `SIGTERM` signals, ensuring:

* Scraper stops cleanly
* Final statistics are logged
* Servers are shut down without leaks

---

## ğŸ§‘â€ğŸ’» Author & License

Created by Mohamed Abbas Homani
Licensed under the MIT License