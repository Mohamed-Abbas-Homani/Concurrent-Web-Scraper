# ğŸ•·ï¸ Concurrent Web Scraper Microservice

A high-performance, configurable microservice for concurrent web scraping. It exposes three interfaces:

* ğŸŒ A **user-friendly web page** for viewing and submitting scraping tasks
* ğŸ“¡ A **gRPC API** for fast, typed interactions
* ğŸ“¥ A **RESTful HTTP API** via Echo server for browser/client-side access

---

## ğŸš€ Features

* âš™ï¸ **Concurrent Scraping** with configurable concurrency, timeouts, and retries
* ğŸ“Š **Statistics & Metrics** tracking processed, successful, and failed tasks
* ğŸ“¦ **In-memory Caching** to reduce redundant scraping
* ğŸ”— **Multiple Selectors per Task** for fine-grained extraction
* ğŸ§ª **gRPC & REST APIs** for integration and control
* ğŸ–¥ï¸ **Interactive Web Interface** for task submission and result viewing

---

## ğŸ“¸ Screenshots

### ğŸ–¥ï¸ Web Interface

![Web Interface](screenshots/web.png)

---

### ğŸ“¡ gRPC Testing (e.g., with `grpcurl` or Postman)

![gRPC API](screenshots/grpc.png)

---

### ğŸŒ Echo Server â€“ REST API in Browser

![Echo Server](screenshots/echo.png)

---

### ğŸ§° CLI Output

![CLI Output](screenshots/cli.png)

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.MD
â”œâ”€â”€ bin
â”‚   â””â”€â”€ scraper
â”œâ”€â”€ cmd
â”‚   â””â”€â”€ scraper
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ config.json
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ internal
â”‚   â”œâ”€â”€ cache
â”‚   â”‚   â””â”€â”€ cache.go
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ config.go
â”‚   â”œâ”€â”€ ratelimit
â”‚   â”‚   â””â”€â”€ ratelimit.go
â”‚   â”œâ”€â”€ scraper
â”‚   â”‚   â””â”€â”€ scraper.go
â”‚   â””â”€â”€ server
â”‚       â”œâ”€â”€ grpc.go
â”‚       â””â”€â”€ http.go
â”œâ”€â”€ pkg
â”‚   â””â”€â”€ types
â”‚       â”œâ”€â”€ proto
â”‚       â””â”€â”€ types.go
â”œâ”€â”€ screenshots
â”‚   â”œâ”€â”€ cli.png
â”‚   â”œâ”€â”€ echo.png
â”‚   â”œâ”€â”€ grpc.png
â”‚   â””â”€â”€ web.png
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ echo_server_test.go
â”‚   â”œâ”€â”€ grpc_server_test.go
â”‚   â””â”€â”€ scraper_test.go
â””â”€â”€ web
    â””â”€â”€ index.html

```

---

## âš™ï¸ Installation & Usage

### ğŸ”¨ Build the project

```bash
make build
```

### âœ… Run tests

```bash
make test
```

### ğŸ³ Build & Run in Docker

```bash
make docker-build
make docker-run
```

* gRPC API available at `localhost:8080`
* REST API (Echo server) at `localhost:8081`

---

## ğŸ§ª API Endpoints

### gRPC

* **Service**: `ScraperService`
* **Methods**:

  * `AddTask(ScrapingTask)`
  * `GetResults(Empty)`

> Define your gRPC client using `pkg/types/proto/scraper.proto`

### REST (Echo)

* `POST /tasks` â€“ Add new scraping task
* `GET /results` â€“ Get completed results

---

## ğŸ“„ Scraping Task Structure

```json
{
  "url": "http://example.com",
  "selectors": {
    "title": "h1",
    "description": ".desc"
  },
  "headers": {
    "Authorization": "Bearer token"
  },
  "method": "GET"
}
```

---

## ğŸ“¦ Configuration

Supports JSON config via `-config` flag:

```json
{
  "max_concurrency": 10,
  "rate_limit": 2.5,
  "timeout": "5s",
  "retry_attempts": 3,
  "retry_delay": "1s",
  "cache_ttl": "30m",
  "user_agent": "MyScraperBot/1.0",
  "max_body_size": 1048576
}
```

Or use built-in defaults.

---

## ğŸ“œ Makefile Targets

| Target              | Description                             |
| ------------------- | --------------------------------------- |
| `make all`          | Clean, format, test, build              |
| `make build`        | Build the binary in `bin/scraper`       |
| `make test`         | Run all tests                           |
| `make fmt`          | Format Go code                          |
| `make clean`        | Remove binaries                         |
| `make proto`        | Generate Go code from proto definitions |
| `make docker-build` | Build Docker image                      |
| `make docker-run`   | Run container with ports exposed        |

---

## ğŸ›  Development

To regenerate gRPC stubs:

```bash
make proto
```

---

## ğŸ§¹ Graceful Shutdown

Handles `SIGINT` and `SIGTERM` signals, ensuring:

* Scraper stops cleanly
* Final statistics are logged
* Servers are shut down without leaks

---

## ğŸ§‘â€ğŸ’» Author & License

Created by Mohamed Abbas Homani
Licensed under the MIT License